{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\n#from datetime import datetime\nfrom scipy.signal import butter, deconvolve\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\nfrom math import sqrt\nfrom multiprocessing import cpu_count\nfrom joblib import Parallel\nfrom joblib import delayed\nfrom warnings import catch_warnings\nfrom warnings import filterwarnings\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom sklearn.metrics import mean_squared_error\nfrom numpy import array\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom matplotlib import pyplot\nfrom pandas.plotting import autocorrelation_plot\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport matplotlib.dates as mdates\nimport seaborn as sns\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"state=pd.read_csv('/kaggle/input/corona/covid_19_india.csv')\nstate['Date'] = state['Date'].astype('datetime64[ns]')\nstate[\"date\"]=state[\"Date\"].dt.date\n\nconfirms=state.groupby([\"date\"])[\"Confirmed\"].sum().reset_index()\ndeads=state.groupby([\"date\"])[\"Deaths\"].sum().reset_index()\ncures=state.groupby([\"date\"])[\"Cured\"].sum().reset_index()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def exp_smoothing_forecast(history, config):\n\tt,d,s,p,b,r = config\n\t# define model model\n\thistory = array(history)\n\tmodel = ExponentialSmoothing(history, trend=t, damped=d, seasonal=s, seasonal_periods=p)\n\t# fit model\n\tmodel_fit = model.fit(optimized=True, use_boxcox=b, remove_bias=r)\n\t# make one step forecast\n\tyhat = model_fit.predict(len(history), len(history))\n\treturn yhat[0]\n\ndef train_test_split(data, n_test):\n\treturn data[:-n_test], data[-n_test:]\n\ndef measure_rmse(actual, predicted):\n\treturn sqrt(mean_squared_error(actual, predicted))\n\ndef walk_forward_validation(data, n_test, cfg):\n\tpredictions = list()\n\t# split dataset\n\ttrain, test = train_test_split(data, n_test)\n\t# seed history with training dataset\n\thistory = [x for x in train]\n\t# step over each time-step in the test set\n\tfor i in range(len(test)):\n\t\t# fit model and make forecast for history\n\t\tyhat = exp_smoothing_forecast(history, cfg)\n\t\t# store forecast in list of predictions\n\t\tpredictions.append(yhat)\n\t\t# add actual observation to history for the next loop\n\t\thistory.append(test[i])\n\t# estimate prediction error\n\terror = measure_rmse(test, predictions)\n\treturn error\n\ndef score_model(data, n_test, cfg, debug=False):\n\tresult = None\n\t# convert config to a key\n\tkey = str(cfg)\n\t# show all warnings and fail on exception if debugging\n\tif debug:\n\t\tresult = walk_forward_validation(data, n_test, cfg)\n\telse:\n\t\t# one failure during model validation suggests an unstable config\n\t\ttry:\n\t\t\t# never show warnings when grid searching, too noisy\n\t\t\twith catch_warnings():\n\t\t\t\tfilterwarnings(\"ignore\")\n\t\t\t\tresult = walk_forward_validation(data, n_test, cfg)\n\t\texcept:\n\t\t\terror = None\n\t# check for an interesting result\n\tif result is not None:\n\t\tprint(' > Model[%s] %.3f' % (key, result))\n\treturn (key, result)\n\ndef grid_search(data, cfg_list, n_test, parallel=True):\n\tscores = None\n\tif parallel:\n\t\t# execute configs in parallel\n\t\texecutor = Parallel(n_jobs=cpu_count(), backend='multiprocessing')\n\t\ttasks = (delayed(score_model)(data, n_test, cfg) for cfg in cfg_list)\n\t\tscores = executor(tasks)\n\telse:\n\t\tscores = [score_model(data, n_test, cfg) for cfg in cfg_list]\n\t# remove empty results\n\tscores = [r for r in scores if r[1] != None]\n\t# sort configs by error, asc\n\tscores.sort(key=lambda tup: tup[1])\n\treturn scores\n\ndef exp_smoothing_configs(seasonal=[None]):\n\tmodels = list()\n\t# define config lists\n\tt_params = ['add', 'mul', None]\n\td_params = [True, False]\n\ts_params = ['add', 'mul', None]\n\tp_params = seasonal\n\tb_params = [True, False]\n\tr_params = [True, False]\n\t# create config instances\n\tfor t in t_params:\n\t\tfor d in d_params:\n\t\t\tfor s in s_params:\n\t\t\t\tfor p in p_params:\n\t\t\t\t\tfor b in b_params:\n\t\t\t\t\t\tfor r in r_params:\n\t\t\t\t\t\t\tcfg = [t,d,s,p,b,r]\n\t\t\t\t\t\t\tmodels.append(cfg)\n\treturn models\n\ndata = np.array(confirms[\"Confirmed\"])\nprint(data)\n   \t# data split\nn_test = 7\n   \t# model configs\ncfg_list = exp_smoothing_configs()\n   \t# grid search\nscores = grid_search(data, cfg_list, n_test)\nprint('done')\n   \t# list top 3 configs\nfor cfg, error in scores[:3]:\n    print(cfg, error)\n    \nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"def exp_smoothing_forecasting(data, config,n):\n\tt,d,s,p,b,r = config\n\t# define model model\n\thistory = array(data)\n\tmodel = ExponentialSmoothing(history, trend=t, damped=d, seasonal=s, seasonal_periods=p)\n\t# fit model\n\tmodel_fit = model.fit(optimized=True, use_boxcox=b, remove_bias=r)\n\t# make one step forecast\n\ty_pred = model_fit.predict(len(data), len(data)+n-1)\n\treturn y_pred\n\ny_pred_7=exp_smoothing_forecasting(confirms[\"Confirmed\"],['mul', True, None, None, False, False],7)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Forecasting of next 7 days**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nstart_date = confirms['date'].max()\nprediction_dates = []\nfor i in range(7):\n    date = start_date + datetime.timedelta(days=1)\n    prediction_dates.append(date)\n    start_date = date\n\nfor i in range(7):\n    print(prediction_dates[i], y_pred_7[i])    \nfig=plt.figure(figsize= (15,10))\nplt.xlabel(\"Dates\",fontsize = 20)\nplt.ylabel('Total cases',fontsize = 20)\nplt.title(\"Predicted Values for the next 7 Days\" , fontsize = 20)\nplt.plot_date(y= y_pred_7,x= prediction_dates,linestyle ='dashed',color = '#ff9999',label = 'Predicted')\nplt.plot_date(y=confirms['Confirmed'],x=confirms['date'],linestyle = '-',color = 'blue',label = 'Actual')\nfig.autofmt_xdate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = np.array(confirms[\"Confirmed\"])\nprint(data)\n   \t# data split\nn_test = 14\n   \t# model configs\ncfg_list = exp_smoothing_configs()\n   \t# grid search\nscores = grid_search(data, cfg_list, n_test)\nprint('done')\n   \t# list top 3 configs\nfor cfg, error in scores[:3]:\n    print(cfg, error)\n    \nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Forecast of 14 days**"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"y_pred_14=exp_smoothing_forecasting(confirms[\"Confirmed\"],['mul', True, None, None, False, False],14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nstart_date = confirms['date'].max()\nprediction_dates = []\nfor i in range(14):\n    date = start_date + datetime.timedelta(days=1)\n    prediction_dates.append(date)\n    start_date = date\nfor i in range(14):\n    print(prediction_dates[i], y_pred_14[i])\nfig=plt.figure(figsize= (15,10))\nplt.xlabel(\"Dates\",fontsize = 20)\nplt.ylabel('Total cases',fontsize = 20)\nplt.title(\"Predicted Values for the next 14 Days\" , fontsize = 20)\nplt.plot_date(y= y_pred_14,x= prediction_dates,linestyle ='dashed',color = '#ff9999',label = 'Predicted')\nplt.plot_date(y=confirms['Confirmed'],x=confirms['date'],linestyle = ':',color = 'blue',label = 'Actual')\nplt.legend()\nfig.autofmt_xdate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autocorrelation_plot(confirms[\"Confirmed\"])\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate an ARIMA model for a given order (p,d,q)\ndef evaluate_arima_model(X, arima_order):\n\t# prepare training dataset\n\ttrain_size = int(len(X) * 0.7)\n\ttrain, test = X[0:train_size], X[train_size:]\n\thistory = [x for x in train]\n\t# make predictions\n\tpredictions = list()\n\tfor t in range(len(test)):\n\t\tmodel = ARIMA(history, order=arima_order)\n\t\tmodel_fit = model.fit(disp=0)\n\t\tyhat = model_fit.forecast()[0]\n\t\tpredictions.append(yhat)\n\t\thistory.append(test[t])\n\t# calculate out of sample error\n\terror = mean_squared_error(test, predictions)\n\treturn error\n \n# evaluate combinations of p, d and q values for an ARIMA model\ndef evaluate_models(dataset, p_values, d_values, q_values):\n\tdataset = dataset.astype('float32')\n\tbest_score, best_cfg = float(\"inf\"), None\n\tfor p in p_values:\n\t\tfor d in d_values:\n\t\t\tfor q in q_values:\n\t\t\t\torder = (p,d,q)\n\t\t\t\ttry:\n\t\t\t\t\tmse = evaluate_arima_model(dataset, order)\n\t\t\t\t\tif mse < best_score:\n\t\t\t\t\t\tbest_score, best_cfg = mse, order\n\t\t\t\t\tprint('ARIMA%s MSE=%.3f' % (order,mse))\n\t\t\t\texcept:\n\t\t\t\t\tcontinue\n\tprint('Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))\n \n# load dataset\n\nseries = confirms[\"Confirmed\"]\n# evaluate parameters\np_values = [1,2,3,4,5,6,7,8,9,10,11]\nd_values = range(0, 7)\nq_values = range(0, 7)\nwarnings.filterwarnings(\"ignore\")\nevaluate_models(series.values, p_values, d_values, q_values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#we will predict using the predictions \ndef evaluate_arima_model(X, arima_order,n):\n\t# prepare training dataset\n\t#train_size = int(len(X) * 0.7)\n\ttrain = X\n\thistory = [x for x in train]\n\t# make predictions\n\tpredictions = list()\n\tfor t in range(n):\n\t\tmodel = ARIMA(history, order=arima_order)\n\t\tmodel_fit = model.fit(disp=0)\n\t\tyhat = model_fit.forecast()[0]\n\t\tpredictions.append(yhat)\n\t\thistory.append(predictions[t])\n\t# calculate out of sample error\n\t\n\treturn predictions\npred=evaluate_arima_model(confirms[\"Confirmed\"].values,[1,2,0],7)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport datetime\nstart_date = confirms['date'].max()\nprediction_dates = []\nfor i in range(7):\n    date = start_date + datetime.timedelta(days=1)\n    prediction_dates.append(date)\n    start_date = date\nfor i in range(7):\n    print(prediction_dates[i], pred[i])\nfig=plt.figure(figsize= (15,10))\nplt.xlabel(\"Dates\",fontsize = 20)\nplt.ylabel('Total cases',fontsize = 20)\nplt.title(\"Predicted Values for the next 7 Days using Arima\" , fontsize = 20)\nplt.plot_date(y=pred,x= prediction_dates,linestyle ='dashed',color = '#ff9999',label = 'Predicted')\nplt.plot_date(y=confirms['Confirmed'],x=confirms['date'],linestyle = '-',color = 'blue',label = 'Actual')\nfig.autofmt_xdate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npred_14=evaluate_arima_model(confirms[\"Confirmed\"].values,[1,2,0],14)\nimport datetime\nstart_date = confirms['date'].max()\nprediction_dates = []\nfor i in range(14):\n    date = start_date + datetime.timedelta(days=1)\n    prediction_dates.append(date)\n    start_date = date\nfor i in range(14):\n    print(prediction_dates[i], pred_14[i])\nfig=plt.figure(figsize= (15,10))\nplt.xlabel(\"Dates\",fontsize = 20)\nplt.ylabel('Total cases',fontsize = 20)\nplt.title(\"Predicted Values for the next 7 Days using Arima\" , fontsize = 20)\nplt.plot_date(y=pred_14,x= prediction_dates,linestyle ='dashed',color = '#ff9999',label = 'Predicted')\nplt.plot_date(y=confirms['Confirmed'],x=confirms['date'],linestyle = '-',color = 'blue',label = 'Actual')\nfig.autofmt_xdate()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using recurrent neural networks"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nsc=MinMaxScaler()\nt=np.array(confirms[\"Confirmed\"])\nt=np.reshape(t,(-1,1))\nX=sc.fit_transform(t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=[]\nfor i in range(21):\n    X_train = []\n    X_test=[]\n    y_train = []\n    for i in range(7, len(X)):\n        X_train.append(X[i-7:i, 0])\n        y_train.append(X[i, 0])\n    \n    X_test.append(X[len(X)-7:len(X),0])\n    X_test=np.array(X_test)\n    X_train, y_train = np.array(X_train), np.array(y_train)\n    #y_train=np.reshape(y_train,(len(y_train),1))\n     \n    \n    #X_test=X_train[-1]\n    #X_train=np.delete(X_train,-1,axis=0)\n    #y_train=np.delete(y_train,-1,axis=0)\n    #X_test=np.reshape(X_test,(1,7))\n# Reshaping\n    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n\n    from keras.models import Sequential\n    from keras.layers import Dense\n    from keras.layers import LSTM\n    from keras.layers import Dropout\n    \n    # Initialising the RNN\n    regressor = Sequential()\n    \n    # Adding the first LSTM layer and some Dropout regularisation\n    regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n    regressor.add(Dropout(0.2))\n    \n    # Adding a second LSTM layer and some Dropout regularisation\n    regressor.add(LSTM(units = 50, return_sequences = True))\n    regressor.add(Dropout(0.1))\n    \n    # Adding a third LSTM layer and some Dropout regularisation\n    regressor.add(LSTM(units = 50, return_sequences = True))\n    regressor.add(Dropout(0.1))\n    \n    # Adding a fourth LSTM layer and some Dropout regularisation\n    regressor.add(LSTM(units = 50))\n    regressor.add(Dropout(0.1))\n    \n    # Adding the output layer\n    regressor.add(Dense(units = 1))\n    \n    # Compiling the RNN\n    regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n    \n    # Fitting the RNN to the Training set\n    regressor.fit(X_train, y_train, epochs = 100, batch_size = 25)\n    p=regressor.predict(X_test)\n    \n    X=np.append(X,p)\n    X=np.reshape(X,(-1,1))\n    p=sc.inverse_transform(p)\n    pred.append(p)\n    \n    ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}